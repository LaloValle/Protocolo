\hfill \break
\justifying
Apectos como los gestos, muecas, sonidos, etc. son factores influyentes en la comunicación del estado consciente o inconsciente de una persona que lo hace visible para ser interpretable por los demás. Sin embargo en la comunicación humana, aunque relevantes estos aspectos, es inevitable reconocer al lenguaje como el elemento simbólico fundamental en la interacción entre individuos y grupos.

\hfill \break
\justifying
El lenguaje humano como la prueba de especímenes que han desarrollado un intelecto sofisticado, ha sido en la historia de la evolución humana la piedra angular de la que nuestra especie se ha valido como herramienta para generar conocimiento y perpetuarlo para ser compartido y adquirido por generaciones posteriores, que ha permitido la comunicación para la organización de grupos con un objetivo en común, incluso como el medio para la expresión de sentimientos y pensamientos en forma de literatura y poemas.

\hfill \break
\justifying
A nivel oral, expresamos el lenguaje mediante el habla, alcanzado en algún punto de la evolución del sistema canal vocal-auditivo en el humano, derivado del descenso de la laringe y lo que nos dio la posibilidad de crear sonidos[5]. A través de la especialización como especie en la actividad del habla, se fueron otorgando semántica a los sonidos generados y nos permitió la asociación de un significado a estos[5].

\hfill \break
\justifying
Es entonces sencillo visualizar la importancia del lenguaje, resaltando especificamente el lenguaje oral como tema de estudio en este trabajo, pues es la herramienta cotidiana utilizada por la gran mayoría de las personas para la comunicación e interacción social. Contrastado por un porcentaje muy mayor de personas que no cuentan con ninguna circunstancia que le impida el habla, un sector de la población en la sociedad se ve limitado en el aprovechamiento de este recurso humano(Siendo 2,234,303 personas, un aproximado del 1.76\% de la población total en México para el Censo 2020 para personas con dificultades o discapacidad para hablar o comunicarse[6]), originado por trastornos y afecciones que dificultan, o incluso impiden e imposibilitan, que una persona tenga la capacidad de expresarse verbalmente.
Entre las afecciones más habituales se encuentran 3 de ellas que podrían verse beneficiadas por el uso de la solución propuesta en este trabajo.

\hfill \break
\justifying
Las primeras son las \textbf{afasias}[7-9], las cuales son problemas médico originado por una lesión cerebral y que resulta en la pérdida o alteración del lenguaje.  Las \textbf{apraxias}[7,11,12] que son trastornos neurológicos caracterizado por la pérdida de la capacidad de llevar a cabo movimientos diestros y gestos, aún cuando se tenga el deseo y la habilidad física para hacerlo, teniendo diferentes afectaciones en función de la parte lesionada en el cerebro. Finalmente existe la \textbf{disartria}[7,13,14] que es un trastorno de la ejecución motora del habla debido a un problema neurológico por la presencia de un accidente u lesiones cerebrales. Afecta gravemente la motricidad de los músculos para el habla. Importante mencionar que en algunos casos los profesionales médicos recomiendan el uso de algún dispositivo electrónico o tecnológico de apoyo para la comunicación para las disartrias.[14]

\hfill \break
\justifying
Otro tipo de trastorno que también afecta de forma indirecta a la capacidad para la expresión hablada del lenguaje como consecuencia de la afección principal, es la sordera de percepción total y es clasificado como un trastorno de la audición. Aún cuando este proyecto propuesto podría ser útil en algunas situaciones para sujetos con este trastorno, sus condiciones limitan el uso y por lo tanto no es el principal sector de la población considerada para beneficiarse del trabajo. Este tipo de afecciones requiere de consideraciones adicionales que garanticen la comunicación bidireccional entre un individuo con sordera y otro sin trastornos auditivos, esto con el objetivo de proponer una tecnología atractiva y útil para su uso por el público objetivo.

\hfill \break
\justifying
Frente a esta problemática, se propone en este documento el desarrollo de una solución tecnológica basada en el SoC micro:bit, que permita el sensado del movimiento de una mano utilizando el acelerometro integrado, y que a partir de unos patrones de movimientos predefinidos con un sencillo código propuesto, se puedan identificar letras del alfabeto reproducibles sonoramente con sus respectivos fonemas del idioma Español usado en México.
Para conseguir la reproducción de letras, sílabas, palabras y hasta frases, los patrones de movimiento del código propuesto se procesan en una etapa conformada por el algoritmo DTW y un modelo clasificador de \textit{Machine Learning}, para la identificación de las respectivas clases alfabéticas, y que a través de la concatenación de cada movimiento identificado se conformen palabras. En una última etapa después del ensamblado de palabras y frases, la reproducción sonora se logra consultando un servicio de \textit{Text-to-Speech} como el proyecto \textit{TTS de Mozilla} en un ambiente local y servicios especializados en la nube como \textit{Microsoft Azure Text-to-speech}.

\hfill \break
\justifying
En este sentido y debido a la aparente utilidad e innovación en el uso de guantes de traducción para la comunicación entre personas con afectaciones en el habla y audición con la sociedad en general, se han desarrollado cantidad de trabajos a escala internacional, nacional e incluso interinstitucional basándose principalmente en el lenguaje de señas respectivo del país donde se investigó.

%
%	Estado del arte
%
\hfill \break
\justifying
Anteriormente se han desarrollados trabajos de titulación enfocados en sistemas de apoyo para la comunicación de personas con afecciones en el habla y que se encargan de realizar una traducción del LSM(Lenguaje de Señas Mexicano) al español mediante la reproducción sonora de las letras o palabras. La solución propuesta en el trabajo \textit{Guante traductor de señas para sordomudos}[1], se basa precisamente en un prototipo de guante equipado con sensores de flexión en los dedos y que son procesados y controlados mediante un $\mu C$ que enlazado a un sintetizador de voz y una pequeña pantalla LCD se reproduce el mensaje identificado; Este guante se encuentra limitado a 26 letras del abecedario y algunas abreviaturas.

\hfill \break
\justifying
Similar al trabajo anterior, pero empleando técnicas de Visión Artificial o Visión por Computadora, el trabajo "\textit{Sistema de comunicación auditiva para personas con problemas del habla}"[2], utiliza la tecnología infrarroja del dispositivo Kinect desarrollado por Microsoft para la obtención de imágenes con las que al aplicarse un algoritmo de clasificación de características en las familias de \textit{Random Forest} y \textit{Mean Shift}, son implementadas en complemento con un modelo de Redes Neuronales para obtener los resultados de las señales realizadas por el usuario.

\hfill \break
\justifying
En el ámbito internacional se han desarrollado trabajos enfocados en miniaturizar y disminuir el hardware necesario en la creación de un \textit{wearable} sensor del movimiento de las manos y los dedos en forma de una pulsera[3]. En este trabajo desarrollado por un equipo de la Universidad de Amrita en la India, utilizan en conjunto un sensor IMU(\textit{Inertial Measurement Unit}) integra un par de sensores: acelerómetro y giroscopio, junto con un arreglo de electrodos EMG(\textit{Electromyography}) que permiten reconocer la contracción de los músculos para el movimiento respectivo de cada dedo.

\hfill \break
\justifying
En otro trabajo realizado por un equipo de 3 investigadores en el \textit{National institute of Technology Puducherry Karaikal India}[4], se propone un prototipo de guante que implementa sensores de flexión, acelerómetro y giroscopio compilando sus mediciones con un Arduino Nano y que permite el envio de la recopilación vía Bluetooth a una PC que corre un algoritmo de ML para la clasificación de los gestos, SVM(\textit{Support Vector Machine}). Este prototipo además permite la identificación de gestos correspondientes al Lenguaje de Señas Americano(ASL) y el Lenguaje de Señas Indio(ISL).

\hfill \break
\justifying
De los trabajos realizados y que se encuentran relacionados a la solución que se ofrece en este trabajo, se aprecia que las principales aplicaciones suelen desarrollarse bajo el área de la Visión por Computadora; Lo que acarrea sus complicaciones debido al específico ambiente de contraste e iluminación que requieren las técnicas de análisis de imágenes, así como su poca portabilidad e intuitivo uso. Como así también los prototipos de guantes o \textit{wearables}, que principalmente encuentran sus inconvenientes en la cantidad de hardware y estética, para un extendido uso como herramientas tecnológicas de apoyo en la comunicación.

\hfill \break
\justifying
La solución propuesta se decanta por un prototipo físico tipo guante \textit{wearable}, aunque más sencillo que los generados por trabajos anteriormente mencionados, y con una implementación limitada de sensores que se traduce directamente en menos hardware, esto aporta a su vez mayor portabilidad y sencillez en el uso del prototipo. Se opta también por utilizar un código de patrones de movimiento especialmente diseñado para el funcionamiento con este sistema embebido, divergiendo de los desarrollos que comunmente emplean el lenguaje de señas del país donde se ha investigado.
Las razones principales por las que el código propuesto se utiliza en vez del LSM(Lenguaje de Señas Mexicano) aún cuando este trabajo tiene el objetivo común de permitir la comunicación principalmente de una comunidad con impedimento del habla, es por los siguientes aspectos en los que difieren primordialmente ambos:

\begin{itemize}
	\item El LSM es un lenguaje que requiere para su expresión el involucramiento de gestos con movimiento desde encima de la cabeza y hasta debajo de la cadera, incluyendo movimiento de manos, expresiones faciales y mirada intencional. Por su parte el código motriz que se propondrá se limita únicamente a 1 mano, donde se colocará el SoC para el sensado.
	
	\item La interpretación de ambos se realiza en dominios sensoriales humanos distintos. Completamente visual para el LSM, y parcialmente visual para el código propuesto, pero principalmente auditivo.
	
	\item Otra diferencia importante se encuentra en la complejidad y relación con el idioma español. Mientras el LSM cuenta con una rica y compleja gramática, y vocabulario abundante, también existe desacoplado de la estructura, gramática y normas propias del español. El enfoque principal del código que se propondrá, es la expresión fonética de letras y palabras del Español, sobrepasando sus capacidades la formulación de normas gramaticales o vocabulario propio, fungiendo únicamente como un simple codificado motriz para la expresión sonora del Español.
	
	\item Finalmente su implementación, aún cuando es común para las personas con impedimento del habla u afecciones relacionadas a la expresión oral, el LSM brinda mayores posibilidades para la población sordo-muda. Mientras tanto el prototipo de este trabajo podrá ser poco beneficioso para la población sordo-muda frente al LSM, pero propone una alternativa para la comunicación(dentro de los límites de este trabajo, aún unilateral) entre personas con impedimento del habla hacia la población con afecciones relacionadas a la percepción visual.
\end{itemize}






